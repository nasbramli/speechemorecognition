# -*- coding: utf-8 -*-
"""SpeechEmoRecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ags-6tfJuG5fLB2Vdwib39FPBOTkxjC

# Setting Up Files
"""

from google.colab import drive
drive.mount('/content/drive/')

path_dataset_drive = '/content/drive/MyDrive/Python Data/SER/archive.zip'

!mkdir dataset

!cd dataset
!unzip '/content/drive/MyDrive/Python Data/SER/archive.zip' -d '/content/dataset'

path_dataset = '/content/dataset'

import os
import pandas as pd
import numpy as np

path_list, label_list = [], []
for folder in sorted(os.listdir(path_dataset)):
  for f in sorted(os.listdir(os.path.join(path_dataset,folder))):
    temp_path= os.path.join(path_dataset,folder)
    temp_path = os.path.join(temp_path, str(f))
    if temp_path[-4:] == '.wav':
      path_list += temp_path,
      f = int(f[6:8])
      label_list += f,

path_list[-1] # sample path for cross checking

path_list[-1][-24:]

label_list[-1] # Checking to see if we got correct emotion

"""# Audio Feture Extraction"""

import librosa
import soundfile
import os, glob, pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

emotions={
  '01':'neutral',
  '02':'calm',
  '03':'happy',
  '04':'sad',
  '05':'angry',
  '06':'fearful',
  '07':'disgust',
  '08':'surprised'
}
#DataFlair - Emotions to observe
observed_emotions=['neutral', 'sad', 'angry', 'calm',
                   'happy', 'fearful', 'disgust', 'surprised']

!pip install pydub --quiet

from pydub import AudioSegment

def extract_feature(file_name, mfcc, chroma, mel):
    with soundfile.SoundFile(file_name) as sound_file:
        X = sound_file.read(dtype="float32")
        sample_rate=sound_file.samplerate
        if chroma:
            stft=np.abs(librosa.stft(X))
        result=np.array([])
        if mfcc:
            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
            result=np.hstack((result, mfccs))
        if chroma:
            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
            result=np.hstack((result, chroma))
        if mel:
            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)
            result=np.hstack((result, mel))
        return result

def load_data(path_list, emotions, observed_emotions, test_size=0.2):
    x,y=[],[]
    for file in glob.glob("/content/dataset/Actor_*/*.wav"):
        file_name=os.path.basename(file)
        sound = AudioSegment.from_wav(file)
        sound = sound.set_channels(1)
        sound.export(file, format="wav")
        emotion=emotions[file_name.split("-")[2]]
        if emotion not in observed_emotions:
            continue
        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)
        x.append(feature)
        y.append(emotion)
    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)

x_train,x_test,y_train,y_test=load_data(path_list, emotions, observed_emotions, test_size=0.20)

"""# Model"""

# x_train,x_test,y_train,y_test

output_dict = {
  'neutral':0,
  'calm':1,
  'happy':2,
  'sad':3,
  'angry':4,
  'fearful':5,
  'disgust':6,
  'surprised':7
}

def convert_y(y, output_dict):
  for idx,i in enumerate(y):
    y[idx] = output_dict[y[idx]]
  return np.array(y)

y_train_c = convert_y(y_train, output_dict)
y_test_c = convert_y(y_test, output_dict)

def convert_x(x):
  for idx,i in enumerate(x):
    x[idx] = np.array(i)
  return x

x_train = convert_x(x_train)
x_test = convert_x(x_test)

import tensorflow as tf

input_shape = x_train.shape[1]

def build_model(input_shape):
    input_shape = (input_shape,) #Input Size if equal to no. of columns

    ser_model = tf.keras.models.Sequential()

    ser_model.add(tf.keras.layers.Dense(units = 2048, activation='relu',
                                        input_shape=input_shape))
    ser_model.add(tf.keras.layers.Dropout(0.1)),
    ser_model.add(tf.keras.layers.Dense(units=2048, activation='relu')),
    ser_model.add(tf.keras.layers.Dropout(0.3)),
    ser_model.add(tf.keras.layers.Dense(units=8, activation='softmax'))

    opt = tf.keras.optimizers.Adam()
    #Compiling the model with optimizer, loss and metric to br tracked
    ser_model.compile(optimizer=opt,
                loss='SparseCategoricalCrossentropy',
                metrics=['accuracy'])

    return ser_model

ser_model = build_model(input_shape)

ser_model.summary()

stop_early = tf.keras.callbacks.EarlyStopping(
                            monitor='val_loss', patience=40)
checkpoint = tf.keras.callbacks.ModelCheckpoint(
                        'ser_model.hdf5',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_loss')

history = ser_model.fit(x=x_train,y= np.array(y_train_c),
              validation_data=(x_test, np.array(y_test_c)),
              epochs=1000,
              callbacks=[stop_early, checkpoint])

# 128x2, 64x3, 32x0 - 59.72%
# 256x1 - 52.43%
# 512x1 - 56.60%
# 2048x1 - 69.44%
# 4096x1 - 69.10%
# 2048x1, 64x1 - 67.71%
# 2048x1, 128x1 - 65.28%
# 2048x1, 256x1 - 69.10%
# 2048x1, 512x1 - 69.79%
# 2048x1, 2048x1, dropout(0.2) - 69.44% - Training - 90%
# 2048x1, 2048x1, dropout(0.4) - 68.40% - Training - 80%
# 1024x1, 2048x1, dropout(0.5) - 69.79% - Training - 92%

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(history.history['loss']))

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

print(f'the Accuracy of the model is {max(val_acc)}')